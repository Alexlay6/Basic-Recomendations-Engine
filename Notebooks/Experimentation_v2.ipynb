{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union, List, Tuple, Any, Callable, Dict\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is an expansion of the last notebook, the aim was to get a better eval metric and a potential broader model that can recommend for more books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexj\\AppData\\Local\\Temp\\ipykernel_18560\\2046703773.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  book_df = pd.read_csv('../Data/Upgraded_data/Books.csv',  sep=';', quotechar='\"', header=0, encoding='latin1', error_bad_lines=False, warn_bad_lines=True)\n",
      "C:\\Users\\alexj\\AppData\\Local\\Temp\\ipykernel_18560\\2046703773.py:1: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  book_df = pd.read_csv('../Data/Upgraded_data/Books.csv',  sep=';', quotechar='\"', header=0, encoding='latin1', error_bad_lines=False, warn_bad_lines=True)\n",
      "b'Skipping line 6452: expected 8 fields, saw 9\\nSkipping line 43667: expected 8 fields, saw 10\\nSkipping line 51751: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 92038: expected 8 fields, saw 9\\nSkipping line 104319: expected 8 fields, saw 9\\nSkipping line 121768: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 144058: expected 8 fields, saw 9\\nSkipping line 150789: expected 8 fields, saw 9\\nSkipping line 157128: expected 8 fields, saw 9\\nSkipping line 180189: expected 8 fields, saw 9\\nSkipping line 185738: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 209388: expected 8 fields, saw 9\\nSkipping line 220626: expected 8 fields, saw 9\\nSkipping line 227933: expected 8 fields, saw 11\\nSkipping line 228957: expected 8 fields, saw 10\\nSkipping line 245933: expected 8 fields, saw 9\\nSkipping line 251296: expected 8 fields, saw 9\\nSkipping line 259941: expected 8 fields, saw 9\\nSkipping line 261529: expected 8 fields, saw 9\\n'\n",
      "C:\\Users\\alexj\\AppData\\Local\\Temp\\ipykernel_18560\\2046703773.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  book_df = pd.read_csv('../Data/Upgraded_data/Books.csv',  sep=';', quotechar='\"', header=0, encoding='latin1', error_bad_lines=False, warn_bad_lines=True)\n",
      "C:\\Users\\alexj\\AppData\\Local\\Temp\\ipykernel_18560\\2046703773.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  book_ratings_df = pd.read_csv('../Data/Upgraded_data/Book-Ratings.csv', sep=';', quotechar='\"', header=0, encoding='latin1', error_bad_lines=False, warn_bad_lines=True)\n",
      "C:\\Users\\alexj\\AppData\\Local\\Temp\\ipykernel_18560\\2046703773.py:2: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  book_ratings_df = pd.read_csv('../Data/Upgraded_data/Book-Ratings.csv', sep=';', quotechar='\"', header=0, encoding='latin1', error_bad_lines=False, warn_bad_lines=True)\n",
      "C:\\Users\\alexj\\AppData\\Local\\Temp\\ipykernel_18560\\2046703773.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  users_df = pd.read_csv('../Data/Upgraded_data/Users.csv', sep=';', quotechar='\"', header=0, encoding='latin1', error_bad_lines=False, warn_bad_lines=True)\n",
      "C:\\Users\\alexj\\AppData\\Local\\Temp\\ipykernel_18560\\2046703773.py:3: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  users_df = pd.read_csv('../Data/Upgraded_data/Users.csv', sep=';', quotechar='\"', header=0, encoding='latin1', error_bad_lines=False, warn_bad_lines=True)\n"
     ]
    }
   ],
   "source": [
    "book_df = pd.read_csv('../Data/Upgraded_data/Books.csv',  sep=';', quotechar='\"', header=0, encoding='latin1', error_bad_lines=False, warn_bad_lines=True)\n",
    "book_ratings_df = pd.read_csv('../Data/Upgraded_data/Book-Ratings.csv', sep=';', quotechar='\"', header=0, encoding='latin1', error_bad_lines=False, warn_bad_lines=True)\n",
    "users_df = pd.read_csv('../Data/Upgraded_data/Users.csv', sep=';', quotechar='\"', header=0, encoding='latin1', error_bad_lines=False, warn_bad_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = book_df.join(book_ratings_df.groupby('ISBN').count()['Book-Rating'], on='ISBN')\n",
    "book_df = book_df.rename(columns={\"Book-Rating\": \"Book-Rating-Count\"})\n",
    "book_df = book_df.join(book_ratings_df.groupby('ISBN').mean()['Book-Rating'], on='ISBN')\n",
    "book_df = book_df[book_df['Year-Of-Publication'] != 'DK Publishing Inc']\n",
    "book_df = book_df[book_df['Year-Of-Publication'] != 'Gallimard']\n",
    "book_df['Year-Of-Publication'] = book_df['Year-Of-Publication'].astype(int)\n",
    "book_df = book_df[book_df['Year-Of-Publication']>=1900]\n",
    "book_df = book_df[book_df['Year-Of-Publication']<=2022]\n",
    "book_df = book_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df.to_csv('../Data/Clean_book_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Book-Rating-Count</th>\n",
       "      <th>Book-Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271355</th>\n",
       "      <td>0440400988</td>\n",
       "      <td>There's a Bat in Bunk Five</td>\n",
       "      <td>Paula Danziger</td>\n",
       "      <td>1988</td>\n",
       "      <td>Random House Childrens Pub (Mm)</td>\n",
       "      <td>http://images.amazon.com/images/P/0440400988.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0440400988.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0440400988.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271356</th>\n",
       "      <td>0525447644</td>\n",
       "      <td>From One to One Hundred</td>\n",
       "      <td>Teri Sloat</td>\n",
       "      <td>1991</td>\n",
       "      <td>Dutton Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0525447644.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0525447644.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0525447644.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271357</th>\n",
       "      <td>006008667X</td>\n",
       "      <td>Lily Dale : The True Story of the Town that Ta...</td>\n",
       "      <td>Christine Wicker</td>\n",
       "      <td>2004</td>\n",
       "      <td>HarperSanFrancisco</td>\n",
       "      <td>http://images.amazon.com/images/P/006008667X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/006008667X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/006008667X.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271358</th>\n",
       "      <td>0192126040</td>\n",
       "      <td>Republic (World's Classics)</td>\n",
       "      <td>Plato</td>\n",
       "      <td>1996</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0192126040.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0192126040.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0192126040.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271359</th>\n",
       "      <td>0767409752</td>\n",
       "      <td>A Guided Tour of Rene Descartes' Meditations o...</td>\n",
       "      <td>Christopher  Biffle</td>\n",
       "      <td>2000</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>http://images.amazon.com/images/P/0767409752.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0767409752.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0767409752.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265531 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN                                         Book-Title  \\\n",
       "0       0195153448                                Classical Mythology   \n",
       "1       0002005018                                       Clara Callan   \n",
       "2       0060973129                               Decision in Normandy   \n",
       "3       0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4       0393045218                             The Mummies of Urumchi   \n",
       "...            ...                                                ...   \n",
       "271355  0440400988                         There's a Bat in Bunk Five   \n",
       "271356  0525447644                            From One to One Hundred   \n",
       "271357  006008667X  Lily Dale : The True Story of the Town that Ta...   \n",
       "271358  0192126040                        Republic (World's Classics)   \n",
       "271359  0767409752  A Guided Tour of Rene Descartes' Meditations o...   \n",
       "\n",
       "                 Book-Author  Year-Of-Publication  \\\n",
       "0         Mark P. O. Morford                 2002   \n",
       "1       Richard Bruce Wright                 2001   \n",
       "2               Carlo D'Este                 1991   \n",
       "3           Gina Bari Kolata                 1999   \n",
       "4            E. J. W. Barber                 1999   \n",
       "...                      ...                  ...   \n",
       "271355        Paula Danziger                 1988   \n",
       "271356            Teri Sloat                 1991   \n",
       "271357      Christine Wicker                 2004   \n",
       "271358                 Plato                 1996   \n",
       "271359   Christopher  Biffle                 2000   \n",
       "\n",
       "                                               Publisher  \\\n",
       "0                                Oxford University Press   \n",
       "1                                  HarperFlamingo Canada   \n",
       "2                                        HarperPerennial   \n",
       "3                                   Farrar Straus Giroux   \n",
       "4                             W. W. Norton &amp; Company   \n",
       "...                                                  ...   \n",
       "271355                   Random House Childrens Pub (Mm)   \n",
       "271356                                      Dutton Books   \n",
       "271357                                HarperSanFrancisco   \n",
       "271358                           Oxford University Press   \n",
       "271359  McGraw-Hill Humanities/Social Sciences/Languages   \n",
       "\n",
       "                                              Image-URL-S  \\\n",
       "0       http://images.amazon.com/images/P/0195153448.0...   \n",
       "1       http://images.amazon.com/images/P/0002005018.0...   \n",
       "2       http://images.amazon.com/images/P/0060973129.0...   \n",
       "3       http://images.amazon.com/images/P/0374157065.0...   \n",
       "4       http://images.amazon.com/images/P/0393045218.0...   \n",
       "...                                                   ...   \n",
       "271355  http://images.amazon.com/images/P/0440400988.0...   \n",
       "271356  http://images.amazon.com/images/P/0525447644.0...   \n",
       "271357  http://images.amazon.com/images/P/006008667X.0...   \n",
       "271358  http://images.amazon.com/images/P/0192126040.0...   \n",
       "271359  http://images.amazon.com/images/P/0767409752.0...   \n",
       "\n",
       "                                              Image-URL-M  \\\n",
       "0       http://images.amazon.com/images/P/0195153448.0...   \n",
       "1       http://images.amazon.com/images/P/0002005018.0...   \n",
       "2       http://images.amazon.com/images/P/0060973129.0...   \n",
       "3       http://images.amazon.com/images/P/0374157065.0...   \n",
       "4       http://images.amazon.com/images/P/0393045218.0...   \n",
       "...                                                   ...   \n",
       "271355  http://images.amazon.com/images/P/0440400988.0...   \n",
       "271356  http://images.amazon.com/images/P/0525447644.0...   \n",
       "271357  http://images.amazon.com/images/P/006008667X.0...   \n",
       "271358  http://images.amazon.com/images/P/0192126040.0...   \n",
       "271359  http://images.amazon.com/images/P/0767409752.0...   \n",
       "\n",
       "                                              Image-URL-L  Book-Rating-Count  \\\n",
       "0       http://images.amazon.com/images/P/0195153448.0...                1.0   \n",
       "1       http://images.amazon.com/images/P/0002005018.0...               14.0   \n",
       "2       http://images.amazon.com/images/P/0060973129.0...                3.0   \n",
       "3       http://images.amazon.com/images/P/0374157065.0...               11.0   \n",
       "4       http://images.amazon.com/images/P/0393045218.0...                1.0   \n",
       "...                                                   ...                ...   \n",
       "271355  http://images.amazon.com/images/P/0440400988.0...                1.0   \n",
       "271356  http://images.amazon.com/images/P/0525447644.0...                1.0   \n",
       "271357  http://images.amazon.com/images/P/006008667X.0...                1.0   \n",
       "271358  http://images.amazon.com/images/P/0192126040.0...                1.0   \n",
       "271359  http://images.amazon.com/images/P/0767409752.0...                1.0   \n",
       "\n",
       "        Book-Rating  \n",
       "0          0.000000  \n",
       "1          4.928571  \n",
       "2          5.000000  \n",
       "3          4.272727  \n",
       "4          0.000000  \n",
       "...             ...  \n",
       "271355     7.000000  \n",
       "271356     4.000000  \n",
       "271357     0.000000  \n",
       "271358     0.000000  \n",
       "271359     0.000000  \n",
       "\n",
       "[265531 rows x 10 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame, threshold: Union[float, int] =0.3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs combined preprocessing on the given DataFrame. It includes cleaning, \n",
    "    formatting, and preparing the data for the KNN model.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame to preprocess.\n",
    "    threshold (float or int): The threshold for dropping columns with NaN values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the ColumnTransformer and the preprocessed DataFrame.\n",
    "\n",
    "    \"\"\"\n",
    "    nan_percentage = df.isna().mean()\n",
    "    df = df.drop(columns=nan_percentage[nan_percentage > threshold].index)\n",
    "    features = ['Year-Of-Publication', 'Book-Rating-Count', 'Book-Rating', 'Publisher', 'Book-Author']\n",
    "    df = df[features]\n",
    "    column_transformer = ColumnTransformer([\n",
    "        ('numerical', StandardScaler(), ['Year-Of-Publication', 'Book-Rating-Count', 'Book-Rating']),\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore'), ['Publisher', 'Book-Author']),\n",
    "    ])\n",
    "\n",
    "    return column_transformer, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('numerical', StandardScaler(),\n",
       "                                                  ['Year-Of-Publication',\n",
       "                                                   'Book-Rating-Count',\n",
       "                                                   'Book-Rating']),\n",
       "                                                 ('categorical',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['Publisher',\n",
       "                                                   'Book-Author'])])),\n",
       "                ('classifier', NearestNeighbors(metric='cosine'))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "column_transformer, processed_df = preprocess_data(book_df[book_df['Book-Rating-Count']>19])\n",
    "\n",
    "knn = Pipeline(steps=[('preprocessor', column_transformer),\n",
    "                      ('classifier', NearestNeighbors(n_neighbors=5, metric='cosine'))])\n",
    "\n",
    "knn.fit(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Models/knn_model_20.pkl', 'wb') as file:\n",
    "    pickle.dump(knn, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendation(model: Pipeline, \n",
    "                        df: pd.DataFrame, \n",
    "                        book_title_or_isbn: str, \n",
    "                        top_n: int = 5) -> Union[List[str], str]:\n",
    "    \"\"\"\n",
    "    Makes book recommendations based on a given book title or ISBN, excluding the input book.\n",
    "\n",
    "    Args:\n",
    "    model (Pipeline): The trained KNN pipeline model.\n",
    "    df (pd.DataFrame): The DataFrame containing the book data.\n",
    "    book_title_or_isbn (str): The book title or ISBN to base recommendations on.\n",
    "    top_n (int): The number of recommendations to return (default is 5).\n",
    "\n",
    "    Returns:\n",
    "    Union[List[str], str]: A list of recommended book titles, excluding the input book, \n",
    "                           or an error message if the book is not found.\n",
    "    \"\"\"\n",
    "    book = df[(df['Book-Title'].str.contains(book_title_or_isbn, na=False, case=False)) |\n",
    "              (df['ISBN'] == book_title_or_isbn)]\n",
    "    if book.empty:\n",
    "        return \"Book not found.\"\n",
    "\n",
    "    index = book.index[0]\n",
    "    preprocessed_book_features = model.named_steps['preprocessor'].transform(df[df.index == index])\n",
    "    distances, indices = model.named_steps['classifier'].kneighbors(preprocessed_book_features, n_neighbors=top_n + 1)\n",
    "    \n",
    "    recommended_indices = indices[0]\n",
    "    recommended_indices = recommended_indices[recommended_indices != index][:top_n]\n",
    "    \n",
    "    recommendations = df.iloc[recommended_indices]['Book-Title'].tolist()\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There's a Bat in Bunk Five\",\n",
       " 'Cat Ate My Gymsuit',\n",
       " 'Zia',\n",
       " 'Many Waters (Laurel Leaf Books)',\n",
       " 'Heroes, Gods and Monsters of Greek Myths']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "book_title_or_isbn = \"There's a Bat in Bunk Five\"\n",
    "recommendations = make_recommendation(knn, book_df, book_title_or_isbn)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_isbn_recommendation(model: Pipeline, \n",
    "                             df: pd.DataFrame, \n",
    "                             book_title_or_isbn: str, \n",
    "                             top_n: int = 5) -> Union[List[str], str]:\n",
    "    \"\"\"\n",
    "    Makes book recommendations based on a given book title or ISBN, excluding the input book.\n",
    "\n",
    "    Args:\n",
    "    model (Pipeline): The trained KNN pipeline model.\n",
    "    df (pd.DataFrame): The DataFrame containing the book data.\n",
    "    book_title_or_isbn (str): The book title or ISBN to base recommendations on.\n",
    "    top_n (int): The number of recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "    Union[List[str], str]: A list of recommended book ISBNs, excluding the input book,\n",
    "                           or an error message if the book is not found.\n",
    "    \"\"\"\n",
    "    book = df[(df['Book-Title'].str.contains(book_title_or_isbn, na=False, case=False)) |\n",
    "              (df['ISBN'] == book_title_or_isbn)]\n",
    "    if book.empty:\n",
    "        return \"Book not found.\"\n",
    "\n",
    "    index = book.index[0]\n",
    "    preprocessed_book_features = model.named_steps['preprocessor'].transform(df[df.index == index])\n",
    "    distances, indices = model.named_steps['classifier'].kneighbors(preprocessed_book_features, n_neighbors=top_n + 1)\n",
    "    \n",
    "    recommended_indices = indices[0]\n",
    "    recommended_indices = recommended_indices[recommended_indices != index][:top_n]\n",
    "    \n",
    "    recommendations = df.iloc[recommended_indices]['ISBN'].tolist()\n",
    "\n",
    "    return recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity(recommendations: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the diversity of the given list of recommendations.\n",
    "\n",
    "    Arguments:\n",
    "    recommendations (List[str]): A list of recommended items.\n",
    "\n",
    "    Returns:\n",
    "    float: The diversity score of the recommendations.\n",
    "    \"\"\"\n",
    "    unique_recommendations = len(set(recommendations))\n",
    "    total_recommendations = len(recommendations)\n",
    "    diversity = unique_recommendations *100/ total_recommendations\n",
    "    return diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_catalog_coverage(recommendations: List[str], catalog: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the catalog coverage of the given recommendations.\n",
    "\n",
    "    Arguments:\n",
    "    recommendations (List[str]): A list of recommended items.\n",
    "    catalog (List[str]): The complete list of items in the catalog.\n",
    "\n",
    "    Returns:\n",
    "    float: The catalog coverage percentage.\n",
    "    \"\"\"\n",
    "    recommended_items = set(recommendations)\n",
    "    catalog_items = set(catalog)\n",
    "    coverage = len(recommended_items.intersection(catalog_items))*100 / len(catalog_items)\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to here is generally exactly the same code with some minor difference in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_titles = book_df.drop_duplicates(subset='ISBN')['Book-Title'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_recommendations(model: Any, test_df: pd.DataFrame, full_df: pd.DataFrame, top_n: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate a list of all recommendations for each item in the test DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    model (Any): The recommendation model to be used.\n",
    "    test_df (DataFrame): The test DataFrame containing the data for which recommendations are to be made.\n",
    "    full_df (DataFrame): The full DataFrame containing all data available for making recommendations.\n",
    "    top_n (int): The number of top recommendations to generate for each item (default is 5).\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of all recommendations.\n",
    "    \"\"\"\n",
    "    all_recommendations = []\n",
    "    for _, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "        book_title_or_isbn = row['isbn'] \n",
    "        recommendations = make_recommendation(model, full_df, book_title_or_isbn, top_n)\n",
    "        try:\n",
    "            \n",
    "            if isinstance(recommendations, list):\n",
    "                all_recommendations.extend(recommendations)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(book_title_or_isbn)\n",
    "        \n",
    "    return all_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [11:52<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "sampled_df = book_df.sample(n=2000, random_state=42)\n",
    "recommendations = get_all_recommendations(knn, sampled_df, book_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I had a limit on amount I could sampe as you can see takes a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity score: 100.0 %\n",
      "Catalog coverage: 0.002105759673859942 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Diversity score: {calculate_diversity(recommendations)} %')\n",
    "print(f'Catalog coverage: {calculate_catalog_coverage(recommendations, unique_titles)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So every recommendation is entirely different, this is interesting, this and the catalog coverage will probably be as a result of the size of the data set that has been created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews = book_ratings_df.groupby('User-ID')['ISBN'].apply(list).to_dict()\n",
    "\n",
    "book_sample = book_ratings_df['ISBN'].drop_duplicates().sample(n=2000).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Recommendations: 100%|██████████| 2000/2000 [08:06<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "book_recommendations = {}\n",
    "for book in tqdm(book_sample, desc=\"Generating Recommendations\"):\n",
    "    book_recommendations[book] = make_isbn_recommendation(knn, book_df, book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(user_reviewed_books: list, recommended_books: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the hit rate, i.e., the proportion of recommended books that were also reviewed by the user.\n",
    "\n",
    "    \n",
    "    Args:\n",
    "    user_reviewed_books (list): List of book IDs reviewed by the user.\n",
    "    recommended_books (list): List of book IDs recommended for a specific book.\n",
    "\n",
    "    Returns:\n",
    "    float: Hit rate for the given user and book.\n",
    "    \"\"\"\n",
    "    hits = sum(book in user_reviewed_books for book in recommended_books)\n",
    "    return hits / len(recommended_books) if recommended_books else 0\n",
    "\n",
    "def overall_hit_rate(users_reviews: dict, book_recommendations: dict) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the overall hit rate for a recommendation system across multiple users and books.\n",
    "\n",
    "    Args:\n",
    "    users_reviews (dict): Dictionary mapping user IDs to lists of reviewed book IDs.\n",
    "    book_recommendations (dict): Dictionary mapping book IDs to lists of recommended book IDs.\n",
    "\n",
    "    Returns:\n",
    "    float: Overall hit rate of the recommendation system.\n",
    "    \"\"\"\n",
    "    total_hit_rate = 0\n",
    "    count = 0\n",
    "\n",
    "    for user, reviewed_books in users_reviews.items():\n",
    "        for book in reviewed_books:\n",
    "            if book in book_recommendations:\n",
    "                hit_rate = calculate_hit_rate(reviewed_books, book_recommendations[book])\n",
    "                total_hit_rate += hit_rate\n",
    "                count += 1\n",
    "\n",
    "    return total_hit_rate / count if count else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2539134783696163"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_hit_rate_value = overall_hit_rate(user_reviews, book_recommendations)\n",
    "overall_hit_rate_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is good, at least 25% of the time the recomendation is one that a person has also reviewed as well. Considering the cold start state of the problem this one of the more useful metrics from this task. Given all this lets create an app that will run this. Please feel free to run the KNN app in the terminal. Finally the rest of this notebook will be abit of sandboxing from myself to see what other models I can try, given the time constraint the evididence will be mainly empirical but the aim is to show some other options for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = users_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>santa monica, california, usa</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>albacete, wisconsin, spain</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>melbourne, victoria, australia</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278848</th>\n",
       "      <td>278849</td>\n",
       "      <td>georgetown, ontario, canada</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278850</th>\n",
       "      <td>278851</td>\n",
       "      <td>dallas, texas, usa</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278851</th>\n",
       "      <td>278852</td>\n",
       "      <td>brisbane, queensland, australia</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278852</th>\n",
       "      <td>278853</td>\n",
       "      <td>stranraer, n/a, united kingdom</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278854</th>\n",
       "      <td>278855</td>\n",
       "      <td>tacoma, washington, united kingdom</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168096 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        User-ID                            Location   Age\n",
       "1             2           stockton, california, usa  18.0\n",
       "3             4           porto, v.n.gaia, portugal  17.0\n",
       "5             6       santa monica, california, usa  61.0\n",
       "9            10          albacete, wisconsin, spain  26.0\n",
       "10           11      melbourne, victoria, australia  14.0\n",
       "...         ...                                 ...   ...\n",
       "278848   278849         georgetown, ontario, canada  23.0\n",
       "278850   278851                  dallas, texas, usa  33.0\n",
       "278851   278852     brisbane, queensland, australia  32.0\n",
       "278852   278853      stranraer, n/a, united kingdom  17.0\n",
       "278854   278855  tacoma, washington, united kingdom  50.0\n",
       "\n",
       "[168096 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location_components(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the country component from a location string.\n",
    "\n",
    "    This function splits the input string by ', ' and attempts to return the third\n",
    "    component, which is assumed to be the country. If the input string does not\n",
    "    contain enough components, it returns 'Unknown'.\n",
    "\n",
    "    Args:\n",
    "    location (str): A location string formatted as 'City, Region, Country'.\n",
    "\n",
    "    Returns:\n",
    "    str: The country component of the location or 'Unknown' if not available.\n",
    "    \"\"\"\n",
    "    components = location.split(', ')\n",
    "\n",
    "    country = components[2] if len(components) > 2 else 'Unknown'\n",
    "    return country\n",
    "\n",
    "users_df['Location'] = users_df.apply(\n",
    "    lambda row: pd.Series(extract_location_components(row['Location'])), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "      <th>Loc_</th>\n",
       "      <th>Loc_ pasig city.</th>\n",
       "      <th>Loc_&amp;#20013;&amp;#22269;</th>\n",
       "      <th>Loc_&amp;#32654;&amp;#22269;</th>\n",
       "      <th>Loc_,</th>\n",
       "      <th>Loc_5057chadwick ct.</th>\n",
       "      <th>Loc_600 083</th>\n",
       "      <th>...</th>\n",
       "      <th>Loc_you listed stroud!)</th>\n",
       "      <th>Loc_ysa</th>\n",
       "      <th>Loc_yu-song</th>\n",
       "      <th>Loc_yugoslavia</th>\n",
       "      <th>Loc_zambia</th>\n",
       "      <th>Loc_zelezny brod</th>\n",
       "      <th>Loc_zimbabwe</th>\n",
       "      <th>Loc_álava</th>\n",
       "      <th>Loc_ä¸­å?½</th>\n",
       "      <th>Loc_öð¹ú</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>usa</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>portugal</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>usa</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>spain</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>australia</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278848</th>\n",
       "      <td>278849</td>\n",
       "      <td>canada</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278850</th>\n",
       "      <td>278851</td>\n",
       "      <td>usa</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278851</th>\n",
       "      <td>278852</td>\n",
       "      <td>australia</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278852</th>\n",
       "      <td>278853</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278854</th>\n",
       "      <td>278855</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168096 rows × 618 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        User-ID        Location   Age  Loc_  Loc_ pasig city.  \\\n",
       "1             2             usa  18.0     0                 0   \n",
       "3             4        portugal  17.0     0                 0   \n",
       "5             6             usa  61.0     0                 0   \n",
       "9            10           spain  26.0     0                 0   \n",
       "10           11       australia  14.0     0                 0   \n",
       "...         ...             ...   ...   ...               ...   \n",
       "278848   278849          canada  23.0     0                 0   \n",
       "278850   278851             usa  33.0     0                 0   \n",
       "278851   278852       australia  32.0     0                 0   \n",
       "278852   278853  united kingdom  17.0     0                 0   \n",
       "278854   278855  united kingdom  50.0     0                 0   \n",
       "\n",
       "        Loc_&#20013;&#22269;  Loc_&#32654;&#22269;  Loc_,  \\\n",
       "1                          0                     0      0   \n",
       "3                          0                     0      0   \n",
       "5                          0                     0      0   \n",
       "9                          0                     0      0   \n",
       "10                         0                     0      0   \n",
       "...                      ...                   ...    ...   \n",
       "278848                     0                     0      0   \n",
       "278850                     0                     0      0   \n",
       "278851                     0                     0      0   \n",
       "278852                     0                     0      0   \n",
       "278854                     0                     0      0   \n",
       "\n",
       "        Loc_5057chadwick ct.  Loc_600 083  ...  Loc_you listed stroud!)  \\\n",
       "1                          0            0  ...                        0   \n",
       "3                          0            0  ...                        0   \n",
       "5                          0            0  ...                        0   \n",
       "9                          0            0  ...                        0   \n",
       "10                         0            0  ...                        0   \n",
       "...                      ...          ...  ...                      ...   \n",
       "278848                     0            0  ...                        0   \n",
       "278850                     0            0  ...                        0   \n",
       "278851                     0            0  ...                        0   \n",
       "278852                     0            0  ...                        0   \n",
       "278854                     0            0  ...                        0   \n",
       "\n",
       "        Loc_ysa  Loc_yu-song  Loc_yugoslavia  Loc_zambia  Loc_zelezny brod  \\\n",
       "1             0            0               0           0                 0   \n",
       "3             0            0               0           0                 0   \n",
       "5             0            0               0           0                 0   \n",
       "9             0            0               0           0                 0   \n",
       "10            0            0               0           0                 0   \n",
       "...         ...          ...             ...         ...               ...   \n",
       "278848        0            0               0           0                 0   \n",
       "278850        0            0               0           0                 0   \n",
       "278851        0            0               0           0                 0   \n",
       "278852        0            0               0           0                 0   \n",
       "278854        0            0               0           0                 0   \n",
       "\n",
       "        Loc_zimbabwe  Loc_álava  Loc_ä¸­å?½  Loc_öð¹ú  \n",
       "1                  0          0           0         0  \n",
       "3                  0          0           0         0  \n",
       "5                  0          0           0         0  \n",
       "9                  0          0           0         0  \n",
       "10                 0          0           0         0  \n",
       "...              ...        ...         ...       ...  \n",
       "278848             0          0           0         0  \n",
       "278850             0          0           0         0  \n",
       "278851             0          0           0         0  \n",
       "278852             0          0           0         0  \n",
       "278854             0          0           0         0  \n",
       "\n",
       "[168096 rows x 618 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_dummies = pd.get_dummies(users_df['Location'], prefix='Loc')\n",
    "location_dummies\n",
    "users_df = pd.concat([users_df, location_dummies], axis=1)\n",
    "users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['Age'] = (users_df['Age'] - users_df['Age'].mean()) / users_df['Age'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = users_df[['Age'] + list(location_dummies.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_similarity = cosine_similarity(user_features.head(20000))\n",
    "demographic_similarity_df = pd.DataFrame(demographic_similarity, index=user_features.head(20000).index, columns=user_features.head(20000).index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc_</th>\n",
       "      <th>Loc_ pasig city.</th>\n",
       "      <th>Loc_&amp;#20013;&amp;#22269;</th>\n",
       "      <th>Loc_&amp;#32654;&amp;#22269;</th>\n",
       "      <th>Loc_,</th>\n",
       "      <th>Loc_5057chadwick ct.</th>\n",
       "      <th>Loc_600 083</th>\n",
       "      <th>Loc_?ú?{</th>\n",
       "      <th>Loc_Unknown</th>\n",
       "      <th>Loc_\\\"n/a\\\"\"</th>\n",
       "      <th>...</th>\n",
       "      <th>Loc_you listed stroud!)</th>\n",
       "      <th>Loc_ysa</th>\n",
       "      <th>Loc_yu-song</th>\n",
       "      <th>Loc_yugoslavia</th>\n",
       "      <th>Loc_zambia</th>\n",
       "      <th>Loc_zelezny brod</th>\n",
       "      <th>Loc_zimbabwe</th>\n",
       "      <th>Loc_álava</th>\n",
       "      <th>Loc_ä¸­å?½</th>\n",
       "      <th>Loc_öð¹ú</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278848</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278850</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278851</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278852</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278854</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168096 rows × 615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Loc_  Loc_ pasig city.  Loc_&#20013;&#22269;  Loc_&#32654;&#22269;  \\\n",
       "1          0                 0                     0                     0   \n",
       "3          0                 0                     0                     0   \n",
       "5          0                 0                     0                     0   \n",
       "9          0                 0                     0                     0   \n",
       "10         0                 0                     0                     0   \n",
       "...      ...               ...                   ...                   ...   \n",
       "278848     0                 0                     0                     0   \n",
       "278850     0                 0                     0                     0   \n",
       "278851     0                 0                     0                     0   \n",
       "278852     0                 0                     0                     0   \n",
       "278854     0                 0                     0                     0   \n",
       "\n",
       "        Loc_,  Loc_5057chadwick ct.  Loc_600 083  Loc_?ú?{  Loc_Unknown  \\\n",
       "1           0                     0            0         0            0   \n",
       "3           0                     0            0         0            0   \n",
       "5           0                     0            0         0            0   \n",
       "9           0                     0            0         0            0   \n",
       "10          0                     0            0         0            0   \n",
       "...       ...                   ...          ...       ...          ...   \n",
       "278848      0                     0            0         0            0   \n",
       "278850      0                     0            0         0            0   \n",
       "278851      0                     0            0         0            0   \n",
       "278852      0                     0            0         0            0   \n",
       "278854      0                     0            0         0            0   \n",
       "\n",
       "        Loc_\\\"n/a\\\"\"  ...  Loc_you listed stroud!)  Loc_ysa  Loc_yu-song  \\\n",
       "1                  0  ...                        0        0            0   \n",
       "3                  0  ...                        0        0            0   \n",
       "5                  0  ...                        0        0            0   \n",
       "9                  0  ...                        0        0            0   \n",
       "10                 0  ...                        0        0            0   \n",
       "...              ...  ...                      ...      ...          ...   \n",
       "278848             0  ...                        0        0            0   \n",
       "278850             0  ...                        0        0            0   \n",
       "278851             0  ...                        0        0            0   \n",
       "278852             0  ...                        0        0            0   \n",
       "278854             0  ...                        0        0            0   \n",
       "\n",
       "        Loc_yugoslavia  Loc_zambia  Loc_zelezny brod  Loc_zimbabwe  Loc_álava  \\\n",
       "1                    0           0                 0             0          0   \n",
       "3                    0           0                 0             0          0   \n",
       "5                    0           0                 0             0          0   \n",
       "9                    0           0                 0             0          0   \n",
       "10                   0           0                 0             0          0   \n",
       "...                ...         ...               ...           ...        ...   \n",
       "278848               0           0                 0             0          0   \n",
       "278850               0           0                 0             0          0   \n",
       "278851               0           0                 0             0          0   \n",
       "278852               0           0                 0             0          0   \n",
       "278854               0           0                 0             0          0   \n",
       "\n",
       "        Loc_ä¸­å?½  Loc_öð¹ú  \n",
       "1                0         0  \n",
       "3                0         0  \n",
       "5                0         0  \n",
       "9                0         0  \n",
       "10               0         0  \n",
       "...            ...       ...  \n",
       "278848           0         0  \n",
       "278850           0         0  \n",
       "278851           0         0  \n",
       "278852           0         0  \n",
       "278854           0         0  \n",
       "\n",
       "[168096 rows x 615 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_age = (30 - users_df['Age'].mean()) / users_df['Age'].std()\n",
    "encoded_location = location_dummies.iloc[0]\n",
    "inference_user_features = np.hstack([normalized_age, encoded_location])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.73553397 -0.77557503  0.89189856 ... -0.18721998 -0.77557503\n",
      "  0.72597536]\n"
     ]
    }
   ],
   "source": [
    "user_similarity_scores = cosine_similarity([inference_user_features], user_features)[0]\n",
    "print(user_similarity_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine is perfect as removes magnitude and shows purely direction based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   171,    220,    251,    482,    539,    678,    689,    721,\n",
       "               729,    825,\n",
       "            ...\n",
       "            278471, 278494, 278585, 278620, 278621, 278712, 278738, 278774,\n",
       "            278806, 278820],\n",
       "           dtype='int64', length=5671)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_users = users_df.index[user_similarity_scores > 0.9]\n",
    "similar_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_users_ratings = book_ratings_df[book_ratings_df['User-ID'].isin(similar_users)]\n",
    "top_books = similar_users_ratings.groupby('ISBN')['Book-Rating'].mean().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISBN\n",
       "0152164502    10.0\n",
       "04492142      10.0\n",
       "0786906162    10.0\n",
       "0786906456    10.0\n",
       "0553381407    10.0\n",
       "Name: Book-Rating, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17577</th>\n",
       "      <td>0152164502</td>\n",
       "      <td>The Coffin Quilt: The Feud between the Hatfiel...</td>\n",
       "      <td>Ann Rinaldi</td>\n",
       "      <td>2001</td>\n",
       "      <td>Gulliver Books Paperbacks</td>\n",
       "      <td>http://images.amazon.com/images/P/0152164502.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0152164502.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0152164502.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37130</th>\n",
       "      <td>0786906456</td>\n",
       "      <td>Soulforge: A Novel (Dragonlance Saga)</td>\n",
       "      <td>Margaret Weis</td>\n",
       "      <td>1998</td>\n",
       "      <td>TSR</td>\n",
       "      <td>http://images.amazon.com/images/P/0786906456.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0786906456.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0786906456.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37161</th>\n",
       "      <td>0786906162</td>\n",
       "      <td>The Dawning of a New Age (Dragonlance Dragons ...</td>\n",
       "      <td>Jean Rabe</td>\n",
       "      <td>1996</td>\n",
       "      <td>TSR</td>\n",
       "      <td>http://images.amazon.com/images/P/0786906162.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0786906162.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0786906162.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68600</th>\n",
       "      <td>0553381407</td>\n",
       "      <td>Toxic Parents: Overcoming Their Hurtful Legacy...</td>\n",
       "      <td>Susan Forward</td>\n",
       "      <td>2002</td>\n",
       "      <td>Bantam</td>\n",
       "      <td>http://images.amazon.com/images/P/0553381407.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0553381407.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0553381407.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ISBN                                         Book-Title  \\\n",
       "17577  0152164502  The Coffin Quilt: The Feud between the Hatfiel...   \n",
       "37130  0786906456              Soulforge: A Novel (Dragonlance Saga)   \n",
       "37161  0786906162  The Dawning of a New Age (Dragonlance Dragons ...   \n",
       "68600  0553381407  Toxic Parents: Overcoming Their Hurtful Legacy...   \n",
       "\n",
       "         Book-Author Year-Of-Publication                  Publisher  \\\n",
       "17577    Ann Rinaldi                2001  Gulliver Books Paperbacks   \n",
       "37130  Margaret Weis                1998                        TSR   \n",
       "37161      Jean Rabe                1996                        TSR   \n",
       "68600  Susan Forward                2002                     Bantam   \n",
       "\n",
       "                                             Image-URL-S  \\\n",
       "17577  http://images.amazon.com/images/P/0152164502.0...   \n",
       "37130  http://images.amazon.com/images/P/0786906456.0...   \n",
       "37161  http://images.amazon.com/images/P/0786906162.0...   \n",
       "68600  http://images.amazon.com/images/P/0553381407.0...   \n",
       "\n",
       "                                             Image-URL-M  \\\n",
       "17577  http://images.amazon.com/images/P/0152164502.0...   \n",
       "37130  http://images.amazon.com/images/P/0786906456.0...   \n",
       "37161  http://images.amazon.com/images/P/0786906162.0...   \n",
       "68600  http://images.amazon.com/images/P/0553381407.0...   \n",
       "\n",
       "                                             Image-URL-L  \n",
       "17577  http://images.amazon.com/images/P/0152164502.0...  \n",
       "37130  http://images.amazon.com/images/P/0786906456.0...  \n",
       "37161  http://images.amazon.com/images/P/0786906162.0...  \n",
       "68600  http://images.amazon.com/images/P/0553381407.0...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_df[book_df.ISBN.isin(top_books.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_similarity_model(sample_df: pd.DataFrame, \n",
    "                          user_features: np.ndarray, \n",
    "                          book_df: pd.DataFrame, \n",
    "                          book_ratings_df: pd.DataFrame, \n",
    "                          users_df: pd.DataFrame, \n",
    "                          threshold: float = 0.99) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a similarity model for book recommendations.\n",
    "\n",
    "    This function iterates over a sample dataframe, calculates user similarity\n",
    "    scores using a cosine similarity metric, and generates book recommendations. \n",
    "    It then calculates the catalog coverage and diversity of these recommendations.\n",
    "\n",
    "    Args:\n",
    "    sample_df (pd.DataFrame): A dataframe containing the sample data.\n",
    "    user_features (np.ndarray): An array of user feature vectors.\n",
    "    book_df (pd.DataFrame): A dataframe containing book data.\n",
    "    book_ratings_df (pd.DataFrame): A dataframe containing user book ratings.\n",
    "    users_df (pd.DataFrame): A dataframe containing user data.\n",
    "    threshold (float): A threshold value for similarity scores. Defaults to 0.99.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float]: A tuple containing the catalog coverage and diversity \n",
    "                         of the recommendations.\n",
    "    \"\"\"\n",
    "    all_recommendations = []\n",
    "    catalog = set(book_df['ISBN'])\n",
    "\n",
    "    for _, encoded_location in tqdm(sample_df.iterrows(),\n",
    "                                    total=sample_df.shape[0]):\n",
    "        normalized_age = ((np.random.randint(1, 100) - \n",
    "                           users_df['Age'].mean()) / \n",
    "                           users_df['Age'].std())\n",
    "  \n",
    "        inference_user_features = np.hstack([normalized_age, \n",
    "                                             encoded_location.values])\n",
    "        user_similarity_scores = cosine_similarity([inference_user_features], \n",
    "                                                   user_features)[0]\n",
    "\n",
    "        similar_users = users_df.index[user_similarity_scores > threshold]\n",
    "        \n",
    "        if not similar_users:\n",
    "            continue\n",
    "        similar_users_ratings = book_ratings_df[book_ratings_df['User-ID']\n",
    "                                                .isin(similar_users)]\n",
    "        top_books = (similar_users_ratings.groupby('ISBN')['Book-Rating']\n",
    "                     .mean().sort_values(ascending=False)\n",
    "                     .head(5).index.tolist())\n",
    "        \n",
    "        all_recommendations.extend(top_books)\n",
    "\n",
    "    catalog_coverage = calculate_catalog_coverage(all_recommendations, catalog)\n",
    "    diversity = calculate_diversity(all_recommendations)\n",
    "\n",
    "    return catalog_coverage, diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [08:43<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "catalog_coverage, diversity = eval_similarity_model(location_dummies.sample(500), user_features, book_df, book_ratings_df, users_df, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity score: 10.12 %\n",
      "Catalog coverage: 0.08438974056603774 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Diversity score: {diversity} %')\n",
    "print(f'Catalog coverage: {catalog_coverage} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the best results this model might need alot of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_engine(age: float, \n",
    "                          encoded_location: np.ndarray, \n",
    "                          user_features: np.ndarray, \n",
    "                          users_df: pd.DataFrame, \n",
    "                          book_ratings_df: pd.DataFrame, \n",
    "                          book_df: pd.DataFrame,\n",
    "                          threshold: float = 0.99) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Generates book recommendations based on normalized age and location encoding.\n",
    "\n",
    "    Arguments:\n",
    "    age (float): Age of the user.\n",
    "    encoded_location (np.ndarray): Encoded location vector of the user.\n",
    "    user_features (np.ndarray): Array of features for all users.\n",
    "    users_df (pd.DataFrame): DataFrame containing user information.\n",
    "    book_ratings_df (pd.DataFrame): DataFrame containing book ratings by users.\n",
    "    book_df (pd.DataFrame): DataFrame containing book information.\n",
    "    threshold (float): Threshold for user similarity (default 0.99).\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], List[str]]: A tuple containing a list of top 5 book titles and \n",
    "                                 their corresponding ISBNs recommended for the user.\n",
    "    \"\"\"\n",
    "    normalised_age = (age - users_df['Age'].mean()) / users_df['Age'].std()\n",
    "    inference_user_features = np.hstack([normalised_age, encoded_location])\n",
    "    user_similarity_scores = cosine_similarity([inference_user_features], \n",
    "                                               user_features)[0]\n",
    "\n",
    "    similar_users = users_df.index[user_similarity_scores > threshold]\n",
    "\n",
    "    if not similar_users:\n",
    "        return [], []\n",
    "\n",
    "    similar_users_ratings = book_ratings_df[book_ratings_df['User-ID']\n",
    "                                            .isin(similar_users)]\n",
    "    top_isbns = (similar_users_ratings.groupby('ISBN')['Book-Rating']\n",
    "                 .mean().sort_values(ascending=False)\n",
    "                 .head(5).index.tolist())\n",
    "    top_books = book_df[book_df['ISBN'].isin(top_isbns)]['Book-Title'].tolist()\n",
    "\n",
    "    return top_books, top_isbns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(age: int, \n",
    "                        country: str, \n",
    "                        locations_encoding: pd.DataFrame, \n",
    "                        recommendation_engine: Callable[[float, np.ndarray, np.ndarray, pd.DataFrame, pd.DataFrame, pd.DataFrame, float], Tuple[List[str], List[str]]], \n",
    "                        user_features: np.ndarray, \n",
    "                        users_df: pd.DataFrame, \n",
    "                        book_ratings_df: pd.DataFrame,\n",
    "                        book_df: pd.DataFrame) -> Union[Tuple[List[str], List[str]], str]:\n",
    "    \"\"\"\n",
    "    Generates book recommendations based on the user's age and country.\n",
    "\n",
    "    Arguments:\n",
    "    age (int): The age of the user.\n",
    "    country (str): The country of the user.\n",
    "    locations_encoding (pd.DataFrame): The table with location encodings.\n",
    "    recommendation_engine (Callable): A function that takes encoded location and \n",
    "                                      age to generate recommendations.\n",
    "    user_features (np.ndarray): Array of user features.\n",
    "    users_df (pd.DataFrame): DataFrame containing user information.\n",
    "    book_ratings_df (pd.DataFrame): DataFrame containing book ratings.\n",
    "    book_df (pd.DataFrame): DataFrame containing book information.\n",
    "\n",
    "    Returns:\n",
    "    Union[Tuple[List[str], List[str]], str]: List of book recommendations and their \n",
    "                                             corresponding ISBNs if inputs are valid, \n",
    "                                             otherwise an error message.\n",
    "    \"\"\"\n",
    "\n",
    "    if not 1 <= age <= 100:\n",
    "        return \"Error: Age must be between 1 and 100.\"\n",
    "\n",
    "    encoded_vector = np.zeros(len(locations_encoding.columns))\n",
    "    formatted_country = f\"Loc_{country.lower()}\"\n",
    "    \n",
    "    if formatted_country in locations_encoding.columns:\n",
    "        encoded_vector[locations_encoding.columns.get_loc(formatted_country)] = 1\n",
    "    else:\n",
    "        return f\"Error: Country '{country}' is not available in the location encodings.\"\n",
    "\n",
    "    recommendations, top_isbns = recommendation_engine(age, encoded_vector, \n",
    "                                                       user_features, users_df, \n",
    "                                                       book_ratings_df, book_df)\n",
    "\n",
    "    return recommendations, top_isbns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Die Brautprinzessin Actics for Market Domination',\n",
       "  \"Early Eight (Working Man's Mystery)\",\n",
       "  'Scarletti Curse (Candleglow)',\n",
       "  'Marin',\n",
       "  'The Gallaudet survival guide to signing'],\n",
       " ['050552421X', '0451212851', '3608953523', '0930323343', '0811800229'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(21, 'usa', location_dummies, recommendation_engine, user_features, users_df, book_ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to try some ages and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Loc_</th>\n",
       "      <th>Loc_ pasig city.</th>\n",
       "      <th>Loc_&amp;#20013;&amp;#22269;</th>\n",
       "      <th>Loc_&amp;#32654;&amp;#22269;</th>\n",
       "      <th>Loc_,</th>\n",
       "      <th>Loc_5057chadwick ct.</th>\n",
       "      <th>Loc_600 083</th>\n",
       "      <th>Loc_?ú?{</th>\n",
       "      <th>Loc_Unknown</th>\n",
       "      <th>...</th>\n",
       "      <th>Loc_you listed stroud!)</th>\n",
       "      <th>Loc_ysa</th>\n",
       "      <th>Loc_yu-song</th>\n",
       "      <th>Loc_yugoslavia</th>\n",
       "      <th>Loc_zambia</th>\n",
       "      <th>Loc_zelezny brod</th>\n",
       "      <th>Loc_zimbabwe</th>\n",
       "      <th>Loc_álava</th>\n",
       "      <th>Loc_ä¸­å?½</th>\n",
       "      <th>Loc_öð¹ú</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193732</th>\n",
       "      <td>-0.953101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71473</th>\n",
       "      <td>-0.537246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223248</th>\n",
       "      <td>-1.022410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249471</th>\n",
       "      <td>-0.398627</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245102</th>\n",
       "      <td>0.987557</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116663</th>\n",
       "      <td>1.749958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197213</th>\n",
       "      <td>1.611340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127983</th>\n",
       "      <td>-0.121390</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213848</th>\n",
       "      <td>-1.299647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64411</th>\n",
       "      <td>-1.022410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>841 rows × 616 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Age  Loc_  Loc_ pasig city.  Loc_&#20013;&#22269;  \\\n",
       "193732 -0.953101     0                 0                     0   \n",
       "71473  -0.537246     0                 0                     0   \n",
       "223248 -1.022410     0                 0                     0   \n",
       "249471 -0.398627     0                 0                     0   \n",
       "245102  0.987557     0                 0                     0   \n",
       "...          ...   ...               ...                   ...   \n",
       "116663  1.749958     0                 0                     0   \n",
       "197213  1.611340     0                 0                     0   \n",
       "127983 -0.121390     0                 0                     0   \n",
       "213848 -1.299647     0                 0                     0   \n",
       "64411  -1.022410     0                 0                     0   \n",
       "\n",
       "        Loc_&#32654;&#22269;  Loc_,  Loc_5057chadwick ct.  Loc_600 083  \\\n",
       "193732                     0      0                     0            0   \n",
       "71473                      0      0                     0            0   \n",
       "223248                     0      0                     0            0   \n",
       "249471                     0      0                     0            0   \n",
       "245102                     0      0                     0            0   \n",
       "...                      ...    ...                   ...          ...   \n",
       "116663                     0      0                     0            0   \n",
       "197213                     0      0                     0            0   \n",
       "127983                     0      0                     0            0   \n",
       "213848                     0      0                     0            0   \n",
       "64411                      0      0                     0            0   \n",
       "\n",
       "        Loc_?ú?{  Loc_Unknown  ...  Loc_you listed stroud!)  Loc_ysa  \\\n",
       "193732         0            0  ...                        0        0   \n",
       "71473          0            0  ...                        0        0   \n",
       "223248         0            0  ...                        0        0   \n",
       "249471         0            0  ...                        0        0   \n",
       "245102         0            0  ...                        0        0   \n",
       "...          ...          ...  ...                      ...      ...   \n",
       "116663         0            0  ...                        0        0   \n",
       "197213         0            0  ...                        0        0   \n",
       "127983         0            0  ...                        0        0   \n",
       "213848         0            0  ...                        0        0   \n",
       "64411          0            0  ...                        0        0   \n",
       "\n",
       "        Loc_yu-song  Loc_yugoslavia  Loc_zambia  Loc_zelezny brod  \\\n",
       "193732            0               0           0                 0   \n",
       "71473             0               0           0                 0   \n",
       "223248            0               0           0                 0   \n",
       "249471            0               0           0                 0   \n",
       "245102            0               0           0                 0   \n",
       "...             ...             ...         ...               ...   \n",
       "116663            0               0           0                 0   \n",
       "197213            0               0           0                 0   \n",
       "127983            0               0           0                 0   \n",
       "213848            0               0           0                 0   \n",
       "64411             0               0           0                 0   \n",
       "\n",
       "        Loc_zimbabwe  Loc_álava  Loc_ä¸­å?½  Loc_öð¹ú  \n",
       "193732             0          0           0         0  \n",
       "71473              0          0           0         0  \n",
       "223248             0          0           0         0  \n",
       "249471             0          0           0         0  \n",
       "245102             0          0           0         0  \n",
       "...              ...        ...         ...       ...  \n",
       "116663             0          0           0         0  \n",
       "197213             0          0           0         0  \n",
       "127983             0          0           0         0  \n",
       "213848             0          0           0         0  \n",
       "64411              0          0           0         0  \n",
       "\n",
       "[841 rows x 616 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(user_features, test_size=0.005)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_count(train_features: pd.DataFrame, \n",
    "                  sample_features: np.ndarray, \n",
    "                  book_ratings_df: pd.DataFrame, \n",
    "                  user_id: Union[int, str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the hit count as a percentage for a recommendation system.\n",
    "\n",
    "    This function compares the top recommended books (based on similarity scores \n",
    "    and user ratings) against the books that the user has already reviewed. It \n",
    "    returns the percentage of recommended books that the user has reviewed.\n",
    "\n",
    "    Args:\n",
    "    train_features (pd.DataFrame): A dataframe containing training feature vectors.\n",
    "    sample_features (np.ndarray): An array of sample feature vectors.\n",
    "    book_ratings_df (pd.DataFrame): A dataframe containing book ratings.\n",
    "    user_id (Union[int, str]): The user ID for whom the hit count is calculated.\n",
    "\n",
    "    Returns:\n",
    "    float: The hit count percentage, which indicates how many of the top recommended \n",
    "           books have been reviewed by the user.\n",
    "    \"\"\"\n",
    "    user_similarity_scores = cosine_similarity([sample_features], \n",
    "                                               train_features)[0]\n",
    "    similar_users = train_features.index[user_similarity_scores > 0.99]\n",
    "\n",
    "    if not similar_users:\n",
    "        return 0.0\n",
    "\n",
    "    similar_users_ratings = book_ratings_df[book_ratings_df['User-ID']\n",
    "                                            .isin(similar_users)]\n",
    "    top_isbns = (similar_users_ratings.groupby('ISBN')['Book-Rating']\n",
    "                 .mean().sort_values(ascending=False)\n",
    "                 .head(5).index.tolist())\n",
    "    user_reviewed_books = set(book_ratings_df[book_ratings_df['User-ID'] == user_id]\n",
    "                              ['ISBN'])\n",
    "    hit_count = sum(isbn in user_reviewed_books for isbn in top_isbns)\n",
    "\n",
    "    return (hit_count / len(top_isbns)) * 100 if top_isbns else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 841/841 [13:27<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Hit Count Percentage: 0.04756242568370987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Processing\")\n",
    "hit_counts = test_df.progress_apply(lambda row: get_hit_count(train_df, row, book_ratings_df, row.name), axis=1)\n",
    "print(\"Average Hit Count Percentage:\", hit_counts.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a good result as this is a percentage, however the sample size is very small (due to computational issues), also to be extra precise we could use the most recent reviews and roll backwards. As mentioned earlier one of my regrets was the computational speed of the recomendation production systesm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a couple of other techniques that given more time I would have liked to trial, I suspect the best model would have been an ensemble of the above methods combined with the following. As these are purely theoretical ideas I have not had time to do a full analysis, however they will have functions availalbe to complete emperical analysis should you be curious to do so. Please feel free to ask me more detail on the following during the interview, I hope that the doc strings alone provide all the required explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(book_ratings_df, book_df, on='ISBN')\n",
    "combined_df = pd.merge(combined_df, users_df, on='User-ID')\n",
    "combined_df['Age'].fillna(combined_df['Age'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User-ID                  int64\n",
       "ISBN                    object\n",
       "Book-Rating              int64\n",
       "Book-Title              object\n",
       "Book-Author             object\n",
       "Year-Of-Publication      int32\n",
       "Publisher               object\n",
       "Image-URL-S             object\n",
       "Image-URL-M             object\n",
       "Image-URL-L             object\n",
       "Book-Rating-Count      float64\n",
       "Book-Rating_y          float64\n",
       "Location                object\n",
       "Age                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = combined_df.rename(columns = {'Book-Rating_x': 'Book-Rating'})\n",
    "combined_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def content_based_filtering(user_input: Dict[str, str], books_df: pd.DataFrame, \n",
    "                            top_n: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Implements content-based filtering for book recommendations.\n",
    "\n",
    "    This function combines book attributes (author, title, publisher) into a single\n",
    "    string for each book, then applies TF-IDF vectorisation. It computes cosine\n",
    "    similarity between the user's preferences and book features to recommend the\n",
    "    top N similar books.\n",
    "\n",
    "    Args:\n",
    "        user_input (Dict[str, str]): User's preferred author, book, and publisher.\n",
    "        books_df (DataFrame): DataFrame of books with 'Book-Author', 'Book-Title',\n",
    "                              and 'Publisher' columns.\n",
    "        top_n (int, optional): Number of top recommendations to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing top N recommended books.\n",
    "    \"\"\"\n",
    "    books_df['combined_features'] = (\n",
    "        books_df['Book-Author'] + \" \" + books_df['Book-Title'] + \" \" + books_df['Publisher']\n",
    "    )\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(books_df['combined_features'])\n",
    "    user_profile = (\n",
    "        user_input['author'] + \" \" + user_input['book'] + \" \" + user_input['publisher']\n",
    "    )\n",
    "    user_profile_vec = tfidf.transform([user_profile])\n",
    "\n",
    "    sim_scores = cosine_similarity(user_profile_vec, tfidf_matrix)\n",
    "    recommended_book_indices = sim_scores[0].argsort()[:-top_n - 1:-1]\n",
    "    recommended_books = books_df.iloc[recommended_book_indices]\n",
    "\n",
    "    return recommended_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def collaborative_filtering_sparse(user_id: int, combined_df: pd.DataFrame, \n",
    "                                   top_n: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Implements collaborative filtering using a sparse matrix approach.\n",
    "\n",
    "    This function creates a user-item interaction matrix from a DataFrame and applies\n",
    "    Truncated Singular Value Decomposition (SVD) for dimensionality reduction. It then\n",
    "    finds users with similar preferences based on cosine similarity and recommends the\n",
    "    top N books preferred by these similar users.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): The user ID for whom the recommendations are to be made.\n",
    "        combined_df (DataFrame): DataFrame containing user IDs, ISBNs, and ratings.\n",
    "        top_n (int, optional): Number of top recommendations to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of ISBNs representing the top N recommended books.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_df['user_id_code'] = combined_df['User-ID'].astype('category').cat.codes\n",
    "    combined_df['isbn_code'] = combined_df['ISBN'].astype('category').cat.codes\n",
    "    user_item_matrix = csr_matrix((combined_df['Book-Rating'], (combined_df['user_id_code'], combined_df['isbn_code'])), \n",
    "                                  shape=(combined_df['User-ID'].nunique(), combined_df['ISBN'].nunique()))\n",
    "\n",
    "    svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "    user_item_matrix_reduced = svd.fit_transform(user_item_matrix)\n",
    "    user_idx = combined_df.loc[combined_df['User-ID'] == user_id, 'user_id_code'].iat[0]\n",
    "\n",
    "    user_vector = user_item_matrix_reduced[user_idx]\n",
    "    similarity_scores = cosine_similarity([user_vector], user_item_matrix_reduced)[0]\n",
    "\n",
    "    similar_users_indices = np.argsort(similarity_scores)[::-1][:top_n+1]\n",
    "    similar_users = combined_df.loc[combined_df['user_id_code'].isin(similar_users_indices), 'User-ID'].unique()\n",
    "\n",
    "    recommended_books = combined_df[combined_df['User-ID'].isin(similar_users) & (combined_df['User-ID'] != user_id)]['ISBN'].unique().tolist()\n",
    "\n",
    "    return recommended_books[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendation(user_input: Dict[str, str], user_id: int, combined_df: pd.DataFrame, \n",
    "                          books_df: pd.DataFrame, top_n: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates book recommendations using a hybrid approach combining content-based \n",
    "    and collaborative filtering methods.\n",
    "\n",
    "    This function first obtains recommendations based on the user's input (author, book, \n",
    "    and publisher preferences) using content-based filtering. It then fetches additional\n",
    "    recommendations using collaborative filtering based on user-item interactions. The \n",
    "    final recommendation list is a combination of both, ensuring a diverse selection.\n",
    "\n",
    "    Args:\n",
    "        user_input (Dict[str, str]): User's preferred author, book, and publisher.\n",
    "        user_id (int): The user ID for collaborative filtering.\n",
    "        combined_df (DataFrame): DataFrame containing user IDs, ISBNs, and ratings for collaborative filtering.\n",
    "        books_df (DataFrame): DataFrame of books for content-based filtering.\n",
    "        top_n (int, optional): Number of top recommendations to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of ISBNs representing the top N recommended books.\n",
    "    \"\"\"\n",
    "    content_recommendations = content_based_filtering(user_input, books_df)\n",
    "    collaborative_recommendations = collaborative_filtering_sparse(user_id, combined_df, top_n)\n",
    "\n",
    "    combined_recommendations = list(set(content_recommendations['ISBN'].tolist() + collaborative_recommendations))\n",
    "    return combined_recommendations[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Filtering\n",
      "Collaborative Filtering with Sparse Matrix\n"
     ]
    }
   ],
   "source": [
    "recommendations = hybrid_recommendation({'author': 'J.K. Rowling', 'book': 'Harry Potter', 'publisher': 'Bloomsbury'},23, combined_df, book_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Book-Rating-Count</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>combined_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>044023722X</td>\n",
       "      <td>A Painted House</td>\n",
       "      <td>John Grisham</td>\n",
       "      <td>2001</td>\n",
       "      <td>Dell Publishing Company</td>\n",
       "      <td>http://images.amazon.com/images/P/044023722X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/044023722X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/044023722X.0...</td>\n",
       "      <td>647.0</td>\n",
       "      <td>3.187017</td>\n",
       "      <td>John Grisham A Painted House Dell Publishing C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>0684872153</td>\n",
       "      <td>Angela's Ashes (MMP) : A Memoir</td>\n",
       "      <td>Frank McCourt</td>\n",
       "      <td>1999</td>\n",
       "      <td>Scribner</td>\n",
       "      <td>http://images.amazon.com/images/P/0684872153.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0684872153.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0684872153.0...</td>\n",
       "      <td>326.0</td>\n",
       "      <td>3.337423</td>\n",
       "      <td>Frank McCourt Angela's Ashes (MMP) : A Memoir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35086</th>\n",
       "      <td>043965548X</td>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (Harr...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>2004</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>http://images.amazon.com/images/P/043965548X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/043965548X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/043965548X.0...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.533333</td>\n",
       "      <td>J.K. Rowling Harry Potter and the Prisoner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45490</th>\n",
       "      <td>0743464117</td>\n",
       "      <td>Band of Brothers : E Company, 506th Regiment, ...</td>\n",
       "      <td>Stephen E. Ambrose</td>\n",
       "      <td>2002</td>\n",
       "      <td>Pocket</td>\n",
       "      <td>http://images.amazon.com/images/P/0743464117.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0743464117.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0743464117.0...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>Stephen E. Ambrose Band of Brothers : E Compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205143</th>\n",
       "      <td>0747561966</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>2003</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>http://images.amazon.com/images/P/0747561966.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0747561966.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0747561966.0...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>J.K. Rowling Harry Potter and the Philosopher'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN                                         Book-Title  \\\n",
       "305     044023722X                                    A Painted House   \n",
       "4578    0684872153                    Angela's Ashes (MMP) : A Memoir   \n",
       "35086   043965548X  Harry Potter and the Prisoner of Azkaban (Harr...   \n",
       "45490   0743464117  Band of Brothers : E Company, 506th Regiment, ...   \n",
       "205143  0747561966           Harry Potter and the Philosopher's Stone   \n",
       "\n",
       "               Book-Author  Year-Of-Publication                Publisher  \\\n",
       "305           John Grisham                 2001  Dell Publishing Company   \n",
       "4578         Frank McCourt                 1999                 Scribner   \n",
       "35086         J.K. Rowling                 2004    Scholastic Paperbacks   \n",
       "45490   Stephen E. Ambrose                 2002                   Pocket   \n",
       "205143        J.K. Rowling                 2003               Bloomsbury   \n",
       "\n",
       "                                              Image-URL-S  \\\n",
       "305     http://images.amazon.com/images/P/044023722X.0...   \n",
       "4578    http://images.amazon.com/images/P/0684872153.0...   \n",
       "35086   http://images.amazon.com/images/P/043965548X.0...   \n",
       "45490   http://images.amazon.com/images/P/0743464117.0...   \n",
       "205143  http://images.amazon.com/images/P/0747561966.0...   \n",
       "\n",
       "                                              Image-URL-M  \\\n",
       "305     http://images.amazon.com/images/P/044023722X.0...   \n",
       "4578    http://images.amazon.com/images/P/0684872153.0...   \n",
       "35086   http://images.amazon.com/images/P/043965548X.0...   \n",
       "45490   http://images.amazon.com/images/P/0743464117.0...   \n",
       "205143  http://images.amazon.com/images/P/0747561966.0...   \n",
       "\n",
       "                                              Image-URL-L  Book-Rating-Count  \\\n",
       "305     http://images.amazon.com/images/P/044023722X.0...              647.0   \n",
       "4578    http://images.amazon.com/images/P/0684872153.0...              326.0   \n",
       "35086   http://images.amazon.com/images/P/043965548X.0...               15.0   \n",
       "45490   http://images.amazon.com/images/P/0743464117.0...               11.0   \n",
       "205143  http://images.amazon.com/images/P/0747561966.0...                3.0   \n",
       "\n",
       "        Book-Rating                                  combined_features  \n",
       "305        3.187017  John Grisham A Painted House Dell Publishing C...  \n",
       "4578       3.337423  Frank McCourt Angela's Ashes (MMP) : A Memoir ...  \n",
       "35086      3.533333  J.K. Rowling Harry Potter and the Prisoner of ...  \n",
       "45490      2.727273  Stephen E. Ambrose Band of Brothers : E Compan...  \n",
       "205143     4.666667  J.K. Rowling Harry Potter and the Philosopher'...  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_df[book_df['ISBN'].isin(recommendations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp39-cp39-win_amd64.whl (2.1 kB)\n",
      "Collecting tensorflow-intel==2.15.0\n",
      "  Downloading tensorflow_intel-2.15.0-cp39-cp39-win_amd64.whl (300.8 MB)\n",
      "     -------------------------------------- 300.8/300.8 MB 5.8 MB/s eta 0:00:00\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "     ------------------------------------- 413.4/413.4 kB 13.0 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.59.3-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 19.6 MB/s eta 0:00:00\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "     ------------------------------------- 442.0/442.0 kB 27.0 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.8.0)\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 22.1 MB/s eta 0:00:00\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 21.5 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 18.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (63.4.1)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-win_amd64.whl (938 kB)\n",
      "     ------------------------------------- 938.4/938.4 kB 29.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.37.1)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.3)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.24.0-py2.py3-none-any.whl (183 kB)\n",
      "     ---------------------------------------- 183.8/183.8 kB ? eta 0:00:00\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.23.4-cp39-cp39-win_amd64.whl (422 kB)\n",
      "     ------------------------------------- 422.5/422.5 kB 27.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.15.0->tensorflow) (3.0.9)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\alexj\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, ml-dtypes, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.24.0 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.3 keras-2.15.0 libclang-16.0.6 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-intel-2.15.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Concatenate, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_enhanced_model(num_users: int, num_books: int, num_publishers: int, \n",
    "                         embedding_size: int, max_text_length: int, \n",
    "                         num_classes: int = 11) -> Model:\n",
    "    \"\"\"\n",
    "    Builds an enhanced neural network model for book recommendation.\n",
    "\n",
    "    This model uses multiple inputs including user, book, publisher, book name, and \n",
    "    publication year to recommend books. Each input type is processed with embedding \n",
    "    layers (or dense layers for text and numeric inputs). The outputs from these layers \n",
    "    are then concatenated and passed through dense layers to produce the final output.\n",
    "\n",
    "    Args:\n",
    "        num_users (int): Total number of unique users.\n",
    "        num_books (int): Total number of unique books.\n",
    "        num_publishers (int): Total number of unique publishers.\n",
    "        embedding_size (int): Size of the embedding vector.\n",
    "        max_text_length (int): Maximum length of the book name text input.\n",
    "        num_classes (int, optional): Number of classes in the output layer. Default is 11.\n",
    "\n",
    "    Returns:\n",
    "        Model: A compiled Keras Model ready for training.\n",
    "\n",
    "    Example:\n",
    "        model = build_enhanced_model(1000, 5000, 300, 50, 25)\n",
    "    \"\"\"\n",
    "    user_input = Input(shape=(1,))\n",
    "    book_input = Input(shape=(1,))\n",
    "    publisher_input = Input(shape=(1,))\n",
    "    book_name_input = Input(shape=(max_text_length,)) \n",
    "    publication_year_input = Input(shape=(1,))\n",
    "\n",
    "    user_embedding = Embedding(num_users, embedding_size, input_length=1)(user_input)\n",
    "    book_embedding = Embedding(num_books, embedding_size, input_length=1)(book_input)\n",
    "    publisher_embedding = Embedding(num_publishers, embedding_size, input_length=1)(publisher_input)\n",
    "\n",
    "    user_vector = Flatten()(user_embedding)\n",
    "    book_vector = Flatten()(book_embedding)\n",
    "    publisher_vector = Flatten()(publisher_embedding)\n",
    "\n",
    "\n",
    "    book_name_vector = Dense(128, activation='relu')(book_name_input)\n",
    "    publication_year_vector = Dense(32, activation='relu')(publication_year_input)\n",
    "\n",
    "    concat = Concatenate()([user_vector, book_vector, publisher_vector, book_name_vector, publication_year_vector])\n",
    "\n",
    "\n",
    "    dense = Dense(256, activation='relu')(concat)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    output = Dense(num_classes, activation='sigmoid')(dense)  \n",
    "\n",
    "    model = Model(inputs=[user_input, book_input, publisher_input, book_name_input, publication_year_input], outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "user_ids = combined_df['User-ID'].astype('category').cat.codes\n",
    "book_ids = combined_df['ISBN'].astype('category').cat.codes\n",
    "publisher_ids = combined_df['Publisher'].astype('category').cat.codes\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(combined_df['Book-Title'])\n",
    "book_titles_seq = tokenizer.texts_to_sequences(combined_df['Book-Title'])\n",
    "max_text_length = max(len(seq) for seq in book_titles_seq)\n",
    "book_titles_padded = pad_sequences(book_titles_seq, maxlen=max_text_length)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "publication_years = scaler.fit_transform(np.array(combined_df['Year-Of-Publication']).reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "X = np.hstack((user_ids.values.reshape(-1, 1), \n",
    "               book_ids.values.reshape(-1, 1), \n",
    "               publisher_ids.values.reshape(-1, 1),\n",
    "               book_titles_padded, \n",
    "               publication_years))\n",
    "\n",
    "num_classes = 11  \n",
    "y = to_categorical(combined_df['Book-Rating'], num_classes) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = combined_df['User-ID'].nunique()\n",
    "num_books = combined_df['ISBN'].nunique()\n",
    "num_publishers = combined_df['Publisher'].nunique()\n",
    "embedding_size = 50\n",
    "\n",
    "model = build_enhanced_model(num_users, num_books, num_publishers, embedding_size, max_text_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "395/395 [==============================] - 65s 160ms/step - loss: 41.6736 - accuracy: 0.5825 - val_loss: 2.0512 - val_accuracy: 0.6267\n",
      "Epoch 2/10\n",
      "395/395 [==============================] - 57s 145ms/step - loss: 1.8619 - accuracy: 0.6282 - val_loss: 1.7316 - val_accuracy: 0.6274\n",
      "Epoch 3/10\n",
      "395/395 [==============================] - 60s 151ms/step - loss: 1.6251 - accuracy: 0.6284 - val_loss: 1.5700 - val_accuracy: 0.6275\n",
      "Epoch 4/10\n",
      "395/395 [==============================] - 59s 150ms/step - loss: 1.5096 - accuracy: 0.6285 - val_loss: 1.4881 - val_accuracy: 0.6276\n",
      "Epoch 5/10\n",
      "395/395 [==============================] - 59s 149ms/step - loss: 1.4502 - accuracy: 0.6285 - val_loss: 1.4467 - val_accuracy: 0.6276\n",
      "Epoch 6/10\n",
      "395/395 [==============================] - 59s 149ms/step - loss: 1.4182 - accuracy: 0.6285 - val_loss: 1.4222 - val_accuracy: 0.6276\n",
      "Epoch 7/10\n",
      "395/395 [==============================] - 59s 149ms/step - loss: 1.4001 - accuracy: 0.6285 - val_loss: 1.4058 - val_accuracy: 0.6276\n",
      "Epoch 8/10\n",
      "395/395 [==============================] - 59s 149ms/step - loss: 1.3882 - accuracy: 0.6285 - val_loss: 1.3958 - val_accuracy: 0.6276\n",
      "Epoch 9/10\n",
      "395/395 [==============================] - 60s 151ms/step - loss: 1.3805 - accuracy: 0.6285 - val_loss: 1.3896 - val_accuracy: 0.6276\n",
      "Epoch 10/10\n",
      "395/395 [==============================] - 60s 152ms/step - loss: 1.3753 - accuracy: 0.6285 - val_loss: 1.3854 - val_accuracy: 0.6276\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit([X_train[:, 0], X_train[:, 1], X_train[:, 2], X_train[:, 3:3+max_text_length], X_train[:, -1]], \n",
    "                    y_train, \n",
    "                    validation_data=([X_test[:, 0], X_test[:, 1], X_test[:, 2], X_test[:, 3:3+max_text_length], X_test[:, -1]], \n",
    "                    y_test), epochs=10, batch_size=2064)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6357/6357 [==============================] - 11s 2ms/step - loss: 1.3854 - accuracy: 0.6276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3853849172592163, 0.6276000142097473]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.evaluate([X_test[:, 0], X_test[:, 1], X_test[:, 2], X_test[:, 3:3+max_text_length], X_test[:, -1]], y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31784/31784 [==============================] - 47s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "user_id = 11676\n",
    "user_books = np.array(list(set(combined_df['ISBN'])))\n",
    "user_books_input = np.array([user_id] * len(user_books))\n",
    "\n",
    "predicted_ratings = model.predict([X[:, 0], X[:, 1], X[:, 2], X[:, 3:3+max_text_length], X[:, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8686687 , 0.07927246, 0.07997542, ..., 0.4842337 , 0.38568106,\n",
       "        0.42214766],\n",
       "       [0.8686687 , 0.07927246, 0.07997542, ..., 0.4842337 , 0.38568106,\n",
       "        0.42214766],\n",
       "       [0.8686687 , 0.07927246, 0.07997542, ..., 0.4842337 , 0.38568106,\n",
       "        0.42214766],\n",
       "       ...,\n",
       "       [0.8686687 , 0.07927246, 0.07997542, ..., 0.4842337 , 0.38568106,\n",
       "        0.42214766],\n",
       "       [0.8686687 , 0.07927246, 0.07997542, ..., 0.4842337 , 0.38568106,\n",
       "        0.42214766],\n",
       "       [0.8686687 , 0.07927246, 0.07997542, ..., 0.4842337 , 0.38568112,\n",
       "        0.42214766]], dtype=float32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1017062, 11)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = np.argmax(predicted_ratings, axis=1)\n",
    "results = pd.DataFrame({'Predictions':final_predictions, 'Title': combined_df['Book-Title'], 'ISBN': combined_df['ISBN']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1017033\n",
       "8           8\n",
       "2           6\n",
       "3           3\n",
       "9           3\n",
       "7           3\n",
       "1           3\n",
       "6           2\n",
       "10          1\n",
       "Name: Predictions, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Title</th>\n",
       "      <th>ISBN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>863903</th>\n",
       "      <td>10</td>\n",
       "      <td>Computer viruses, worms, data diddlers, killer...</td>\n",
       "      <td>031202889X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195478</th>\n",
       "      <td>9</td>\n",
       "      <td>An Elm Creek Quilts Sampler: The First Three N...</td>\n",
       "      <td>074326018X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612418</th>\n",
       "      <td>9</td>\n",
       "      <td>The Know-It-All's Guide to Life: How to Climb ...</td>\n",
       "      <td>1564146731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002877</th>\n",
       "      <td>9</td>\n",
       "      <td>\\And Then Fuzzy Told Seve...\\\": A Collection o...</td>\n",
       "      <td>0809232731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140658</th>\n",
       "      <td>8</td>\n",
       "      <td>The Wicca Book of Days: Legend and Lore for Ev...</td>\n",
       "      <td>0806516852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Predictions                                              Title  \\\n",
       "863903            10  Computer viruses, worms, data diddlers, killer...   \n",
       "195478             9  An Elm Creek Quilts Sampler: The First Three N...   \n",
       "612418             9  The Know-It-All's Guide to Life: How to Climb ...   \n",
       "1002877            9  \\And Then Fuzzy Told Seve...\\\": A Collection o...   \n",
       "140658             8  The Wicca Book of Days: Legend and Lore for Ev...   \n",
       "\n",
       "               ISBN  \n",
       "863903   031202889X  \n",
       "195478   074326018X  \n",
       "612418   1564146731  \n",
       "1002877  0809232731  \n",
       "140658   0806516852  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('Predictions', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Book-Rating-Count</th>\n",
       "      <th>Book-Rating_y</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "      <th>combined_features</th>\n",
       "      <th>user_id_code</th>\n",
       "      <th>isbn_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23886</th>\n",
       "      <td>11676</td>\n",
       "      <td>0446520802</td>\n",
       "      <td>10</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>Nicholas Sparks</td>\n",
       "      <td>1996</td>\n",
       "      <td>Warner Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0446520802.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0446520802.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0446520802.0...</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4.060345</td>\n",
       "      <td>n/a, n/a, n/a</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Nicholas Sparks The Notebook Warner Books</td>\n",
       "      <td>3566</td>\n",
       "      <td>91514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33640</th>\n",
       "      <td>11676</td>\n",
       "      <td>1841764027</td>\n",
       "      <td>10</td>\n",
       "      <td>Wellingtons Peninsula Regiments (1: The Irish ...</td>\n",
       "      <td>Mike Chappell</td>\n",
       "      <td>2003</td>\n",
       "      <td>Osprey Publishing (UK)</td>\n",
       "      <td>http://images.amazon.com/images/P/1841764027.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/1841764027.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/1841764027.0...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>n/a, n/a, n/a</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Mike Chappell Wellingtons Peninsula Regiments ...</td>\n",
       "      <td>3566</td>\n",
       "      <td>233511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27339</th>\n",
       "      <td>11676</td>\n",
       "      <td>0451402065</td>\n",
       "      <td>10</td>\n",
       "      <td>Earth Song</td>\n",
       "      <td>Catherine Coulter</td>\n",
       "      <td>1990</td>\n",
       "      <td>Onyx Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0451402065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0451402065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0451402065.0...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>n/a, n/a, n/a</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Catherine Coulter Earth Song Onyx Books</td>\n",
       "      <td>3566</td>\n",
       "      <td>99945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27332</th>\n",
       "      <td>11676</td>\n",
       "      <td>0425171094</td>\n",
       "      <td>10</td>\n",
       "      <td>A Village Affair</td>\n",
       "      <td>Joanna Trollope</td>\n",
       "      <td>1999</td>\n",
       "      <td>Berkley Publishing Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0425171094.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0425171094.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0425171094.0...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>n/a, n/a, n/a</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Joanna Trollope A Village Affair Berkley Publi...</td>\n",
       "      <td>3566</td>\n",
       "      <td>81032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27320</th>\n",
       "      <td>11676</td>\n",
       "      <td>0452281903</td>\n",
       "      <td>10</td>\n",
       "      <td>A Man Named Dave: A Story of Triumph and Forgi...</td>\n",
       "      <td>David J. Pelzer</td>\n",
       "      <td>2000</td>\n",
       "      <td>Plume Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0452281903.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0452281903.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0452281903.0...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>3.223404</td>\n",
       "      <td>n/a, n/a, n/a</td>\n",
       "      <td>35.0</td>\n",
       "      <td>David J. Pelzer A Man Named Dave: A Story of T...</td>\n",
       "      <td>3566</td>\n",
       "      <td>102761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      User-ID        ISBN  Book-Rating  \\\n",
       "23886   11676  0446520802           10   \n",
       "33640   11676  1841764027           10   \n",
       "27339   11676  0451402065           10   \n",
       "27332   11676  0425171094           10   \n",
       "27320   11676  0452281903           10   \n",
       "\n",
       "                                              Book-Title        Book-Author  \\\n",
       "23886                                       The Notebook    Nicholas Sparks   \n",
       "33640  Wellingtons Peninsula Regiments (1: The Irish ...      Mike Chappell   \n",
       "27339                                         Earth Song  Catherine Coulter   \n",
       "27332                                   A Village Affair    Joanna Trollope   \n",
       "27320  A Man Named Dave: A Story of Triumph and Forgi...    David J. Pelzer   \n",
       "\n",
       "       Year-Of-Publication                 Publisher  \\\n",
       "23886                 1996              Warner Books   \n",
       "33640                 2003    Osprey Publishing (UK)   \n",
       "27339                 1990                Onyx Books   \n",
       "27332                 1999  Berkley Publishing Group   \n",
       "27320                 2000               Plume Books   \n",
       "\n",
       "                                             Image-URL-S  \\\n",
       "23886  http://images.amazon.com/images/P/0446520802.0...   \n",
       "33640  http://images.amazon.com/images/P/1841764027.0...   \n",
       "27339  http://images.amazon.com/images/P/0451402065.0...   \n",
       "27332  http://images.amazon.com/images/P/0425171094.0...   \n",
       "27320  http://images.amazon.com/images/P/0452281903.0...   \n",
       "\n",
       "                                             Image-URL-M  \\\n",
       "23886  http://images.amazon.com/images/P/0446520802.0...   \n",
       "33640  http://images.amazon.com/images/P/1841764027.0...   \n",
       "27339  http://images.amazon.com/images/P/0451402065.0...   \n",
       "27332  http://images.amazon.com/images/P/0425171094.0...   \n",
       "27320  http://images.amazon.com/images/P/0452281903.0...   \n",
       "\n",
       "                                             Image-URL-L  Book-Rating-Count  \\\n",
       "23886  http://images.amazon.com/images/P/0446520802.0...              116.0   \n",
       "33640  http://images.amazon.com/images/P/1841764027.0...                3.0   \n",
       "27339  http://images.amazon.com/images/P/0451402065.0...               20.0   \n",
       "27332  http://images.amazon.com/images/P/0425171094.0...                9.0   \n",
       "27320  http://images.amazon.com/images/P/0452281903.0...               94.0   \n",
       "\n",
       "       Book-Rating_y       Location   Age  \\\n",
       "23886       4.060345  n/a, n/a, n/a  35.0   \n",
       "33640       6.000000  n/a, n/a, n/a  35.0   \n",
       "27339       4.400000  n/a, n/a, n/a  35.0   \n",
       "27332       2.666667  n/a, n/a, n/a  35.0   \n",
       "27320       3.223404  n/a, n/a, n/a  35.0   \n",
       "\n",
       "                                       combined_features  user_id_code  \\\n",
       "23886          Nicholas Sparks The Notebook Warner Books          3566   \n",
       "33640  Mike Chappell Wellingtons Peninsula Regiments ...          3566   \n",
       "27339            Catherine Coulter Earth Song Onyx Books          3566   \n",
       "27332  Joanna Trollope A Village Affair Berkley Publi...          3566   \n",
       "27320  David J. Pelzer A Man Named Dave: A Story of T...          3566   \n",
       "\n",
       "       isbn_code  \n",
       "23886      91514  \n",
       "33640     233511  \n",
       "27339      99945  \n",
       "27332      81032  \n",
       "27320     102761  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df['User-ID']==11676].sort_values('Book-Rating', ascending=False).head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
